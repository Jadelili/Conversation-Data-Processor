# -*- coding: utf-8 -*-
"""Process Conversation Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DlQ2_zdN76hEDhiumUkbon91_jwyjBto
"""

import os
import parselmouth
import pandas as pd
import numpy as np
from pydub import AudioSegment
from scipy.signal import find_peaks
import librosa
import soundfile as sf

def process_textgrid_and_wav(textgrid_path, wav_path, output_dir):
    print(f"Processing TextGrid: {textgrid_path}")
    print(f"Processing WAV: {wav_path}")

    tg = parselmouth.read(textgrid_path)
    wav = AudioSegment.from_wav(wav_path)
    file_basename = os.path.basename(wav_path).replace(".wav", "")
    tier = tg.to_tgt().get_tier_by_name("speaker")  # adjust the name of the conversational partner

    for interval in tier.intervals:
        label = interval.text.strip()
        start_time = interval.start_time * 1000
        end_time = interval.end_time * 1000

        segment = wav[start_time:end_time]

        speaker_type = ""
        if "AI" in label:  # adjust the condition based on your conversational partner, as well as for the following conditions
            speaker_type = "AI"
        elif "human" in label:
            speaker_type = "human"
        elif "speaker" in label:
            speaker_type = "speaker"
        elif "child" in label:
            speaker_type = "child"

        if speaker_type:
            new_file_name = f"{file_basename}_{label}.wav"
            output_path = os.path.join(output_dir, speaker_type, new_file_name)
            segment.export(output_path, format="wav")
            print(f"Exported: {output_path}")

def process_all_files(input_dir, output_dir):
    print(f"Input Directory: {input_dir}")
    print(f"Output Directory: {output_dir}")

    for root, _, files in os.walk(input_dir):
        for file in files:
            if file.endswith(".TextGrid"):
                subject_id = file.replace(".TextGrid", "")
                textgrid_path = os.path.join(root, file)
                wav_path = os.path.join(root, f"{subject_id}.wav")

                if os.path.exists(wav_path):
                    process_textgrid_and_wav(textgrid_path, wav_path, output_dir)
                else:
                    print(f"wav file not found for: {textgrid_path}")

# Method 1 using the librosa library
# first detect syllable nuclei based on intensity peak
def detect_syllable_nuclei(y, sr, silence_threshold=-40):
    y_band = librosa.effects.preemphasis(y, coef=0.97)   # a band-pass filter (300â€“3300 Hz) to focus on human speech frequencies in the background
    y_band = librosa.resample(y_band, orig_sr=sr, target_sr=sr, res_type='kaiser_best')
    intensity = librosa.amplitude_to_db(np.abs(librosa.stft(y_band)), ref=np.max)   # the intensity (in dB) of the audio signal
    avg_intensity_over_time = np.mean(intensity, axis=0)
    avg_intensity = np.mean(avg_intensity_over_time)
    peaks, _ = find_peaks(avg_intensity_over_time, height=avg_intensity + silence_threshold)   # peaks that meet a minimum height above the average intensity
    return len(peaks), peaks

# Method 1 using the librosa library
def analyze_pitch_prosody(audio_path, silence_threshold=-40, minimum_dip_near_peak=2):
    snd = parselmouth.Sound(audio_path)
    pitch = snd.to_pitch()
    intensity = snd.to_intensity()

    pitch_values = pitch.selected_array['frequency']
    valid_pitch = pitch_values[pitch_values > 0]
    mean_pitch = np.mean(valid_pitch) if valid_pitch.size > 0 else 0
    std_pitch = np.std(valid_pitch) if valid_pitch.size > 0 else 0

    intensity_values = intensity.values
    valid_intensity = intensity_values[intensity_values > 0]
    mean_intensity = np.mean(valid_intensity) if valid_intensity.size > 0 else 0
    std_intensity = np.std(valid_intensity) if valid_intensity.size > 0 else 0

    y, sr = librosa.load(audio_path, sr=None)
    duration = librosa.get_duration(y=y, sr=sr)

    syllable_count, peaks = detect_syllable_nuclei(y, sr, silence_threshold)

    silence_intervals = librosa.effects.split(y, top_db=-silence_threshold)
    speaking_time = sum((end - start) / sr for start, end in silence_intervals)
    pause_count = len(silence_intervals) - 1

    syllables_per_second_total = round(syllable_count / duration, 2) if duration > 0 else 0
    syllables_per_second_speaking = round(syllable_count / speaking_time, 2) if speaking_time > 0 else 0
    average_syllable_duration = round(speaking_time / syllable_count, 2) if syllable_count > 0 else 0

    print(f"Processed: {audio_path}")
    return (mean_pitch, std_pitch, mean_intensity, std_intensity,
            syllable_count, pause_count, duration, speaking_time,
            syllables_per_second_total, syllables_per_second_speaking,
            average_syllable_duration)

# Method 2 using the parselmouth and scipy libraries
def analyze_pitch_prosody(audio_path, silence_threshold=30, minimum_dip_near_peak=3):
    snd = parselmouth.Sound(audio_path)
    pitch = snd.to_pitch()
    intensity = snd.to_intensity()

    pitch_values = pitch.selected_array['frequency']
    valid_pitch = pitch_values[pitch_values > 0]
    mean_pitch = round(np.mean(valid_pitch), 2) if valid_pitch.size > 0 else 0
    std_pitch = round(np.std(valid_pitch), 2) if valid_pitch.size > 0 else 0

    intensity_values = intensity.values[0]
    valid_intensity = intensity_values[intensity_values > silence_threshold]
    mean_intensity = round(np.mean(valid_intensity), 2) if valid_intensity.size > 0 else 0
    std_intensity = round(np.std(valid_intensity), 2) if valid_intensity.size > 0 else 0

    duration = round(snd.get_total_duration(), 2)  # (s)
    peaks, _ = find_peaks(intensity_values, height=silence_threshold, prominence=minimum_dip_near_peak,
                            distance=int(0.2 / intensity.get_time_step()))   # syllable peaks in intensity contour
    voiced_syllable_count = len(peaks)
    speaking_frames = intensity_values > silence_threshold
    total_speaking_time = round(np.sum(speaking_frames) * intensity.get_time_step(), 2)  # (s)

    syllables_ps_total = round(voiced_syllable_count / duration, 2) if duration > 0 else 0
    syllables_ps_speaking = round(voiced_syllable_count / total_speaking_time, 2) if total_speaking_time > 0 else 0
    ave_syllable_dur = round(total_speaking_time / voiced_syllable_count, 2) if voiced_syllable_count > 0 else 0

    return mean_pitch, std_pitch, mean_intensity, std_intensity, \
            voiced_syllable_count, duration, total_speaking_time, \
            syllables_ps_total, syllables_ps_speaking, ave_syllable_dur

def process_folder(folder_path, condition):
    results = []
    for file in os.listdir(folder_path):
        if file.endswith('.wav'):
            file_path = os.path.join(folder_path, file)
            mean_pitch, std_pitch, mean_intensity, std_intensity, \
                voiced_syllable_count, duration, total_speaking_time, \
                    syllables_ps_total, syllables_ps_speaking, ave_syllable_dur = analyze_pitch_prosody(file_path)

            results.append({
                'file': file,
                'condition': condition,
                'mean_pitch': mean_pitch,
                'std_pitch': std_pitch,
                'mean_intensity': mean_intensity,
                'std_intensity': std_intensity,
                'voiced_syllable_count': voiced_syllable_count,
                'duration': duration,
                'total_speaking_time': total_speaking_time,
                'syllables_ps_total': syllables_ps_total,
                'syllables_ps_speaking': syllables_ps_speaking,
                'ave_syllable_dur': ave_syllable_dur
            })
    return results

def main(input_directory, output_directory, output_file):
    for speaker in ["AI", "human", "speaker", "child"]:     # adjust to a list of all conversational partners
        os.makedirs(os.path.join(output_directory, speaker), exist_ok=True)

    process_all_files(input_directory, output_directory)

    all_results = []
    for speaker in ["AI", "human", "speaker", "child"]:
        folder_path = os.path.join(output_directory, speaker)
        if not os.path.exists(folder_path):
            print(f"Folder not found: {folder_path}")
            continue

        results = process_folder(folder_path, speaker)
        for result in results:
            result['condition'] = speaker
            all_results.append(result)

    df = pd.DataFrame(all_results)
    df.to_csv(output_file, index=False)
    print(f"Results saved to {output_file}")

input_directory = "the file path to your folder of audio files"
output_directory = "the file path to where you want the segmented turn files to be located"
output_file = "the output csv location"

main(input_directory, output_directory, output_file)

"""# Post-Processing.
Feel free to explore more!
"""

def format_table(input_file, output_file, column):
    df = pd.read_csv(input_file)
    df[['id', 'condition', 'partner', 'turn']] = df[column].str.replace('.wav', '').str.split('_', expand=True)
    df[df.select_dtypes(include=['object']).columns] = df.select_dtypes(include=['object']).apply(lambda x: x.str.strip())
    df['id'] = df['id'].astype(int)
    df['turn'] = df['turn'].astype(int)
    df['condition'] = df['condition'].apply(lambda x: x.capitalize() if x.islower() else x)

    df_sorted = df.drop(columns=column).sort_values(by=['id', 'turn']).reset_index(drop=True)
    df_sorted.to_csv(output_file, index=False)

def stats_box_plot(df, partner):
    x = np.arange(len(df['condition']))

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharex=True)

    pitch_bars = ax1.bar(x, df['mean_speech_rate'],
                        yerr=df['sd_speech_rate'],
                        capsize=5,
                        label='Mean Speech Rate')
    ax1.set_xlabel('Condition')
    ax1.set_ylabel('Mean Speech Rate')
    ax1.set_title(f"Mean Speech Rate of {partner}'s Speech")
    ax1.set_xticks(x)
    ax1.set_xticklabels(df['condition'])
    ax1.legend()

    intensity_bars = ax2.bar(x, df['mean_articulation_rate'],
                            yerr=df['sd_articulation_rate'],
                            capsize=5,
                            label='Mean Articulation Rate')
    ax2.set_xlabel('Condition')
    ax2.set_ylabel('Mean Articulation Rate')
    ax2.set_title(f"Mean Articulation Rate of {partner}'s Speech")
    ax2.set_xticks(x)
    ax2.set_xticklabels(df['condition'])
    ax2.legend(loc='lower right')

    plt.show()